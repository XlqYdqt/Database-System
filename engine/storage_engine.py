#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from typing import List, Dict, Optional, Any, Tuple
import struct

from sql.ast import ColumnDefinition, DataType, ColumnConstraint
from engine.constants import ROW_LENGTH_PREFIX_SIZE
from engine.b_plus_tree import BPlusTree, INVALID_PAGE_ID
from engine.catalog_page import CatalogPage
from engine.table_heap_page import TableHeapPage
from engine.data_page import DataPage
# [FIX] å¼•å…¥ TableNotFoundError å¼‚å¸¸ï¼Œç”¨äºæ›´ç²¾ç¡®çš„é”™è¯¯å¤„ç†
from engine.exceptions import TableAlreadyExistsError, PrimaryKeyViolationError, TableNotFoundError
from storage.buffer_pool_manager import BufferPoolManager
from engine.transaction_manager import TransactionManager


class StorageEngine:
    """
    å­˜å‚¨å¼•æ“ã€‚
    ä½œä¸ºæ•°æ®åº“çš„åº•å±‚æ ¸å¿ƒï¼Œå®ƒè´Ÿè´£ç®¡ç†è¡¨çš„å…ƒæ•°æ®ã€ç‰©ç†å­˜å‚¨ã€ç´¢å¼•ç»´æŠ¤ï¼Œ
    å¹¶å°è£…ä¸ç¼“å†²æ± ç®¡ç†å™¨çš„äº¤äº’ã€‚
    """
    B_PLUS_TREE_KEY_SIZE = 16

    def __init__(self, buffer_pool_manager: BufferPoolManager):
        self.bpm = buffer_pool_manager
        self.indexes: Dict[str, BPlusTree] = {}
        self.txn_manager = TransactionManager(self)

        is_dirty = False
        catalog_page_raw = self.bpm.fetch_page(0)
        if catalog_page_raw is None:
            catalog_page_raw = self.bpm.new_page()
            if catalog_page_raw is None or catalog_page_raw.page_id != 0:
                raise RuntimeError("å…³é”®é”™è¯¯ï¼šç¼“å†²æ± æ— æ³•åˆ†é…æˆ–è·å– page_id=0 ä½œä¸ºç›®å½•é¡µã€‚")

        try:
            if any(b != 0 for b in catalog_page_raw.data):
                self.catalog_page = CatalogPage.deserialize(catalog_page_raw.data)
                is_dirty = False
            else:
                self.catalog_page = CatalogPage()
                catalog_page_raw.data = bytearray(self.catalog_page.serialize())
                is_dirty = True
        finally:
            self.bpm.unpin_page(0, is_dirty)

        self._load_all_indexes()

    def _load_all_indexes(self) -> None:
        """åœ¨ç³»ç»Ÿå¯åŠ¨æ—¶ï¼Œä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰è¡¨çš„B+æ ‘ç´¢å¼•ã€‚"""
        for table_name, metadata in self.catalog_page.tables.items():
            if 'index_root_page_id' in metadata:
                index_root_id = metadata['index_root_page_id']
                self.indexes[table_name] = BPlusTree(self.bpm, index_root_id)

    def _prepare_key_for_b_tree(self, value: Any, col_type: DataType) -> bytes:
        """å°†Pythonå€¼è½¬æ¢ä¸ºB+æ ‘æœŸæœ›çš„ã€å›ºå®šé•¿åº¦ã€å¯æ¯”è¾ƒçš„å­—èŠ‚é”®ã€‚"""
        if col_type == DataType.INT:
            key_bytes = value.to_bytes(8, 'big', signed=True)
        elif col_type in (DataType.TEXT, DataType.STRING):
            key_bytes = str(value).encode('utf-8')
        else:
            raise NotImplementedError(f"ä¸æ”¯æŒçš„ä¸»é”®ç±»å‹ç”¨äºç´¢å¼•: {col_type.name}")

        if len(key_bytes) > self.B_PLUS_TREE_KEY_SIZE:
            raise ValueError(f"ç´¢å¼•é”®è¿‡é•¿ã€‚æœ€å¤§é•¿åº¦ {self.B_PLUS_TREE_KEY_SIZE} å­—èŠ‚ï¼Œå¾—åˆ° {len(key_bytes)} å­—èŠ‚ã€‚")

        return key_bytes.ljust(self.B_PLUS_TREE_KEY_SIZE, b'\x00')

    def _flush_catalog_page(self) -> None:
        """å°†ç›®å½•é¡µï¼ˆCatalogPageï¼‰çš„å†…å®¹åºåˆ—åŒ–å¹¶å¼ºåˆ¶å†™å›ç£ç›˜ã€‚"""
        catalog_page_raw = self.bpm.fetch_page(0)
        if catalog_page_raw:
            try:
                catalog_page_raw.data = bytearray(self.catalog_page.serialize())
                self.bpm.unpin_page(0, True)
            except Exception as e:
                self.bpm.unpin_page(0, False)
                raise e
        else:
            raise RuntimeError("åœ¨ç¼“å†²æ± ä¸­æ‰¾ä¸åˆ°ç›®å½•é¡µï¼Œæ— æ³•åˆ·æ–°ã€‚")

    def get_bplus_tree(self, table_name: str) -> Optional[BPlusTree]:
        """è·å–æŒ‡å®šè¡¨çš„B+æ ‘ç´¢å¼•å¯¹è±¡ã€‚"""
        return self.indexes.get(table_name)

    def create_table(self, table_name: str, columns: List[ColumnDefinition]) -> bool:
        """åˆ›å»ºæ–°è¡¨ã€‚å¦‚æœç¼“å†²æ± å·²æ»¡ï¼Œåˆ™ä¼šæŠ›å‡º MemoryErrorã€‚"""
        if table_name in self.catalog_page.tables:
            raise TableAlreadyExistsError(f"è¡¨ '{table_name}' å·²å­˜åœ¨ã€‚")

        table_heap_page = self.bpm.new_page()
        if not table_heap_page:
            raise MemoryError("ç¼“å†²æ± å·²æ»¡ï¼Œæ— æ³•ä¸ºè¡¨åˆ›å»ºæ–°çš„å †é¡µé¢ã€‚")

        try:
            schema_dict = {col.name: col for col in columns}
            self.catalog_page.add_table(table_name, table_heap_page.page_id, INVALID_PAGE_ID, schema_dict)
            self._flush_catalog_page()
            self.indexes[table_name] = BPlusTree(self.bpm, INVALID_PAGE_ID)
            empty_heap = TableHeapPage()
            table_heap_page.data = bytearray(empty_heap.serialize())
        finally:
            self.bpm.unpin_page(table_heap_page.page_id, True)

        return True

    def insert_row(self, table_name: str, row_data: bytes, txn_id: Optional[int] = None) -> bool:
        """
        æ’å…¥ä¸€è¡Œæ•°æ®ã€‚
        - å¦‚æœ txn_id is Noneï¼šç«‹å³å†™å…¥ï¼ˆéäº‹åŠ¡æ¨¡å¼ï¼‰
        - å¦‚æœ txn_id ä¸ä¸º Noneï¼šå»¶è¿Ÿå†™å…¥ï¼ˆäº‹åŠ¡æ¨¡å¼ï¼Œç­‰åˆ° COMMIT æ—¶æ‰çœŸæ­£å†™ï¼‰
        """
        if txn_id is not None:
            # ğŸš©äº‹åŠ¡æ¨¡å¼ï¼šä¸ç«‹å³å†™å…¥ï¼Œå…ˆè®°å½•åœ¨ write set
            rid_placeholder = ("pending", len(self.txn_manager.transactions[txn_id]['writes']))
            # ç”¨ä¸€ä¸ªè™šæ‹Ÿ rid æ ‡è®°ï¼Œcommit æ—¶å†åˆ†é…çœŸæ­£çš„ RID
            self.txn_manager.add_write_set(
                txn_id,
                table_name,
                rid_placeholder,
                old_data=None,  # æ’å…¥æ“ä½œæ²¡æœ‰æ—§æ•°æ®
                new_data=row_data  # ç¼“å­˜æœªæäº¤çš„æ–°è¡Œ
            )
            print(f"[TXN {txn_id}] Insert scheduled for table '{table_name}', waiting for COMMIT.")
            return True

        # ğŸš©éäº‹åŠ¡æ¨¡å¼ï¼šç«‹å³æ‰§è¡ŒåŸæœ‰é€»è¾‘
        return self._do_insert_immediate(table_name, row_data)

    def _do_insert_immediate(self, table_name: str, row_data: bytes) -> bool:
        table_metadata = self.catalog_page.get_table_metadata(table_name)
        if not table_metadata:
            raise TableNotFoundError(table_name)

        heap_page_id = table_metadata['heap_root_page_id']
        heap_page_raw = self.bpm.fetch_page(heap_page_id)
        if not heap_page_raw:
            raise IOError(f"æ— æ³•ä¸ºè¡¨ '{table_name}' è·å–å †é¡µé¢ {heap_page_id}ã€‚")

        target_page_raw = None
        heap_page_is_dirty = False

        try:
            table_heap = TableHeapPage.deserialize(heap_page_raw.data)

            total_record_length = len(row_data) + ROW_LENGTH_PREFIX_SIZE
            record_to_insert = total_record_length.to_bytes(ROW_LENGTH_PREFIX_SIZE, "little") + row_data

            for page_id in reversed(table_heap.get_page_ids()):
                page_raw = self.bpm.fetch_page(page_id)
                if page_raw:
                    try:
                        data_page = DataPage(page_raw.page_id, page_raw.data)
                        if data_page.get_free_space() >= len(record_to_insert):
                            target_page_raw = page_raw
                            break
                    finally:
                        if target_page_raw is None or page_raw.page_id != target_page_raw.page_id:
                            self.bpm.unpin_page(page_id, False)

            if not target_page_raw:
                target_page_raw = self.bpm.new_page()
                if not target_page_raw:
                    raise MemoryError("ç¼“å†²æ± å·²æ»¡ï¼Œæ— æ³•ä¸ºæ’å…¥åˆ›å»ºæ–°çš„æ•°æ®é¡µã€‚")

                table_heap.add_page_id(target_page_raw.page_id)
                heap_page_raw.data = bytearray(table_heap.serialize())
                heap_page_is_dirty = True

            target_data_page = DataPage(target_page_raw.page_id, target_page_raw.data)
            row_offset = target_data_page.insert_record(record_to_insert)
            target_page_raw.data = bytearray(target_data_page.get_data())

            schema = table_metadata['schema']
            try:
                pk_col_def, pk_col_index = self._get_pk_info(schema)
                pk_value, _ = self._decode_value_from_row(row_data, pk_col_index, schema)
                pk_bytes = self._prepare_key_for_b_tree(pk_value, pk_col_def.data_type)
                rid_tuple = (target_page_raw.page_id, row_offset)

                bplus_tree = self.indexes.get(table_name)
                if bplus_tree is None:
                    return True

                insert_result = bplus_tree.insert(pk_bytes, rid_tuple)

                if insert_result is None:
                    # [TRANSACTION FIX] å…³é”®çš„å›æ»šé€»è¾‘ï¼
                    # å¦‚æœç´¢å¼•æ’å…¥å¤±è´¥ï¼ˆä¸»é”®å†²çªï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»æ’¤é”€åˆšæ‰çš„æ•°æ®æ’å…¥æ“ä½œï¼Œ
                    # å¦åˆ™å°±ä¼šåœ¨æ•°æ®é¡µä¸Šç•™ä¸‹æ²¡æœ‰ç´¢å¼•çš„â€œå¹½çµæ•°æ®â€ã€‚
                    # æˆ‘ä»¬é€šè¿‡é€»è¾‘åˆ é™¤åˆšåˆšæ’å…¥çš„è¡Œæ¥å®ç°å›æ»šã€‚
                    self.delete_row_by_rid(table_name, rid_tuple)
                    raise PrimaryKeyViolationError(pk_value)

                root_changed = insert_result
                if root_changed:
                    self.update_index_root(table_name, bplus_tree.root_page_id)

            except ValueError:
                # è¡¨ä¸­æ²¡æœ‰ä¸»é”®ï¼Œè·³è¿‡ç´¢å¼•æ›´æ–°
                pass

            return True

        finally:
            self.bpm.unpin_page(heap_page_id, heap_page_is_dirty)
            if target_page_raw:
                self.bpm.unpin_page(target_page_raw.page_id, True)


    def scan_table(self, table_name: str) -> List[Tuple[Tuple[int, int], bytes]]:
        """
        æ‰«æå…¨è¡¨ï¼Œè¿”å›æ‰€æœ‰è¡Œæ•°æ®åŠå…¶RIDã€‚
        å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡º TableNotFoundErrorã€‚
        """
        table_metadata = self.catalog_page.get_table_metadata(table_name)
        if not table_metadata:
            raise TableNotFoundError(table_name)

        heap_page_id = table_metadata['heap_root_page_id']
        heap_page_raw = self.bpm.fetch_page(heap_page_id)
        if not heap_page_raw:
            raise IOError(f"æ— æ³•ä¸ºè¡¨ '{table_name}' è·å–å †é¡µé¢ {heap_page_id}ã€‚")

        results = []
        try:
            table_heap = TableHeapPage.deserialize(heap_page_raw.data)
            for data_page_id in table_heap.get_page_ids():
                page_raw = self.bpm.fetch_page(data_page_id)
                if not page_raw: continue
                try:
                    data_page = DataPage(page_raw.page_id, page_raw.data)
                    for offset, record in data_page.get_all_records():
                        rid = (data_page_id, offset)
                        row_data = record[ROW_LENGTH_PREFIX_SIZE:]
                        results.append((rid, row_data))
                finally:
                    self.bpm.unpin_page(data_page_id, False)
        finally:
            self.bpm.unpin_page(heap_page_id, False)

        return results

    def _get_pk_info(self, schema: Dict[str, ColumnDefinition]) -> Tuple[ColumnDefinition, int]:
        """è¾…åŠ©å‡½æ•°ï¼Œä»schemaä¸­è·å–ä¸»é”®çš„å®šä¹‰å’Œåˆ—ç´¢å¼•ä½ç½®ã€‚"""
        for i, col_def in enumerate(schema.values()):
            if any(c[0] == ColumnConstraint.PRIMARY_KEY for c in col_def.constraints):
                return col_def, i
        raise ValueError("è¡¨ä¸­æœªå®šä¹‰ä¸»é”®")

    def _decode_value_from_row(self, row_data: bytes, col_index: int, schema: Dict[str, ColumnDefinition]) -> Tuple[
        Any, int]:
        """æ ¹æ®åˆ—çš„ç´¢å¼•ä»è¡Œæ•°æ®ä¸­è§£ç å‡ºç‰¹å®šä¸€ä¸ªå€¼ã€‚"""
        offset = 0
        for i, col_def in enumerate(schema.values()):
            value, new_offset = self._decode_value(row_data, offset, col_def.data_type)
            if i == col_index:
                return value, new_offset
            offset = new_offset
        raise IndexError("åˆ—ç´¢å¼•è¶…å‡ºèŒƒå›´")

    def read_row(self, table_name: str, rid: Tuple[int, int]) -> Optional[bytes]:
        """æ ¹æ®RIDï¼ˆè®°å½•IDï¼‰è¯»å–å•è¡Œæ•°æ®ã€‚"""
        page_id, offset = rid
        page = self.bpm.fetch_page(page_id)
        if not page: return None
        try:
            data_page = DataPage(page.page_id, page.data)
            record = data_page.get_record(offset)
            return record[ROW_LENGTH_PREFIX_SIZE:] if record else None
        finally:
            self.bpm.unpin_page(page_id, False)

    def _decode_value(self, row_data: bytes, offset: int, col_type: DataType) -> Tuple[Any, int]:
        """æ ¹æ®æ•°æ®ç±»å‹ä»å­—èŠ‚æµçš„æŒ‡å®šåç§»é‡è§£ç ä¸€ä¸ªå€¼ã€‚"""
        try:
            if col_type == DataType.INT:
                value = int.from_bytes(row_data[offset: offset + 4], "little", signed=True)
                offset += 4
            elif col_type in (DataType.TEXT, DataType.STRING):
                length = int.from_bytes(row_data[offset: offset + 4], "little")
                offset += 4
                value = row_data[offset: offset + length].decode("utf-8")
                offset += length
            elif col_type == DataType.FLOAT:
                value, = struct.unpack("<f", row_data[offset: offset + 4])
                offset += 4
            else:
                raise NotImplementedError(f"ä¸æ”¯æŒçš„è§£ç ç±»å‹: {col_type.name}")
            return value, offset
        except (struct.error, IndexError, UnicodeDecodeError) as e:
            raise ValueError(f"ä»åç§»é‡ {offset} è§£ç ç±»å‹ {col_type.name} å¤±è´¥: {e}")

    def delete_row_by_rid(
            self,
            table_name: str,
            rid: Tuple[int, int],
            txn_id: Optional[int] = None
    ) -> bool:
        """
        æ ¹æ®RIDåˆ é™¤ä¸€è¡Œæ•°æ®ï¼ˆé€»è¾‘åˆ é™¤ï¼‰ã€‚
        - å¦‚æœ txn_id is None: ç«‹å³åˆ é™¤ (éäº‹åŠ¡æ¨¡å¼)ã€‚
        - å¦‚æœ txn_id ä¸ä¸º None: å»¶è¿Ÿåˆ é™¤ (äº‹åŠ¡æ¨¡å¼)ã€‚
        """
        page_id, offset = rid
        page = self.bpm.fetch_page(page_id)
        if not page:
            raise IOError(f"æ— æ³•ä¸ºåˆ é™¤æ“ä½œè·å–é¡µé¢ {page_id}ã€‚")

        try:
            data_page = DataPage(page.page_id, page.data)
            old_record = data_page.get_record(offset)
            old_row_data = old_record[ROW_LENGTH_PREFIX_SIZE:] if old_record else None

            if txn_id is not None:
                # ğŸš©äº‹åŠ¡æ¨¡å¼ï¼šåªè®°å½•ï¼Œä¸ç«‹å³åˆ é™¤
                self.txn_manager.add_write_set(
                    txn_id,
                    table_name,
                    rid,
                    old_data=old_row_data,  # å›æ»šæ—¶éœ€è¦é‡æ–°æ’å›
                    new_data=None
                )
                print(f"[TXN {txn_id}] Delete scheduled for table '{table_name}', rid={rid}, waiting for COMMIT.")
                return True

            # ğŸš©éäº‹åŠ¡æ¨¡å¼ï¼šç«‹å³åˆ é™¤
            return self._do_delete_immediate(table_name, rid)

        finally:
            self.bpm.unpin_page(page_id, False)

    def _do_delete_immediate(self, table_name: str, rid: Tuple[int, int]) -> bool:
        """çœŸæ­£æ‰§è¡Œåˆ é™¤ï¼Œç«‹å³ä¿®æ”¹ DataPageã€‚"""
        page_id, offset = rid
        page = self.bpm.fetch_page(page_id)
        if not page:
            raise IOError(f"æ— æ³•ä¸ºåˆ é™¤æ“ä½œè·å–é¡µé¢ {page_id}ã€‚")

        try:
            data_page = DataPage(page.page_id, page.data)
            deleted = data_page.delete_record(offset)
            if deleted:
                page.data = bytearray(data_page.get_data())
            return deleted
        finally:
            self.bpm.unpin_page(page_id, True)

    def update_index_root(self, table_name: str, new_root_id: int) -> None:
        """æ›´æ–°å¹¶æŒä¹…åŒ–ä¸€ä¸ªè¡¨çš„ç´¢å¼•æ ¹é¡µé¢IDã€‚"""
        metadata = self.catalog_page.get_table_metadata(table_name)
        if not metadata:
            raise TableNotFoundError(table_name)
        metadata['index_root_page_id'] = new_root_id
        self._flush_catalog_page()

    def update_row_by_rid(
            self,
            table_name: str,
            rid: Tuple[int, int],
            new_row_data: bytes,
            txn_id: Optional[int] = None
    ) -> Optional[Tuple[int, int]]:
        """
        æ ¹æ®RIDæ›´æ–°ä¸€è¡Œæ•°æ®ã€‚
        - å¦‚æœ txn_id is None: ç«‹å³æ›´æ–° (éäº‹åŠ¡æ¨¡å¼)ã€‚
        - å¦‚æœ txn_id ä¸ä¸º None: å»¶è¿Ÿæ›´æ–° (äº‹åŠ¡æ¨¡å¼)ã€‚
        """
        page_id, old_offset = rid
        page = self.bpm.fetch_page(page_id)
        if not page:
            raise IOError(f"æ— æ³•ä¸ºæ›´æ–°æ“ä½œè·å–é¡µé¢ {page_id}ã€‚")

        try:
            data_page = DataPage(page.page_id, page.data)
            old_record = data_page.get_record(old_offset)
            old_row_data = old_record[ROW_LENGTH_PREFIX_SIZE:] if old_record else None

            if txn_id is not None:
                # ğŸš©äº‹åŠ¡æ¨¡å¼ï¼šåªè®°å½•ï¼Œä¸ç«‹å³å†™å…¥
                self.txn_manager.add_write_set(
                    txn_id,
                    table_name,
                    rid,
                    old_data=old_row_data,  # ç”¨äºå›æ»š
                    new_data=new_row_data  # æäº¤æ—¶åº”ç”¨
                )
                print(f"[TXN {txn_id}] Update scheduled for table '{table_name}', rid={rid}, waiting for COMMIT.")
                return rid  # RID æš‚æ—¶ä¸å˜

            # ğŸš©éäº‹åŠ¡æ¨¡å¼ï¼šç«‹å³æ›´æ–°
            return self._do_update_immediate(table_name, rid, new_row_data)

        finally:
            self.bpm.unpin_page(page_id, False)

    def _do_update_immediate(
                    self,
                    table_name: str,
                    rid: Tuple[int, int],
                    new_row_data: bytes
            ) -> Optional[Tuple[int, int]]:
                """çœŸæ­£æ‰§è¡Œæ›´æ–°ï¼Œç«‹å³å†™ DataPageã€‚"""
                page_id, old_offset = rid
                page = self.bpm.fetch_page(page_id)
                if not page:
                    raise IOError(f"æ— æ³•ä¸ºæ›´æ–°æ“ä½œè·å–é¡µé¢ {page_id}ã€‚")

                try:
                    data_page = DataPage(page.page_id, page.data)
                    total_record_length = len(new_row_data) + ROW_LENGTH_PREFIX_SIZE
                    new_record = total_record_length.to_bytes(ROW_LENGTH_PREFIX_SIZE, 'little') + new_row_data

                    new_offset, moved = data_page.update_record(old_offset, new_record)
                    page.data = bytearray(data_page.get_data())
                    return (page_id, new_offset)
                except (ValueError, IndexError):
                    return None
                finally:
                    self.bpm.unpin_page(page_id, True)



